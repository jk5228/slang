# A lexer generated by lexgen.py. This file is automatically generated.
# Do not edit.

import re, tok

ws = re.compile('\s+')
triples = [('}', '=', re.compile(re.escape('}'))),('||', '=', re.compile(re.escape('||'))),('{', '=', re.compile(re.escape('{'))),('while', '=', re.compile(re.escape('while'))),('return', '=', re.compile(re.escape('return'))),('in', '=', re.compile(re.escape('in'))),('if', '=', re.compile(re.escape('if'))),('for', '=', re.compile(re.escape('for'))),('else', '=', re.compile(re.escape('else'))),('def', '=', re.compile(re.escape('def'))),('break', '=', re.compile(re.escape('break'))),(']', '=', re.compile(re.escape(']'))),('[', '=', re.compile(re.escape('['))),('>=', '=', re.compile(re.escape('>='))),('>', '=', re.compile(re.escape('>'))),('==', '=', re.compile(re.escape('=='))),('=', '=', re.compile(re.escape('='))),('<=', '=', re.compile(re.escape('<='))),('<', '=', re.compile(re.escape('<'))),(';', '=', re.compile(re.escape(';'))),(':', '=', re.compile(re.escape(':'))),('/', '=', re.compile(re.escape('/'))),('...', '=', re.compile(re.escape('...'))),('..', '=', re.compile(re.escape('..'))),('->', '=', re.compile(re.escape('->'))),('-', '=', re.compile(re.escape('-'))),(',', '=', re.compile(re.escape(','))),('+', '=', re.compile(re.escape('+'))),('*', '=', re.compile(re.escape('*'))),(')', '=', re.compile(re.escape(')'))),('(', '=', re.compile(re.escape('('))),('&&', '=', re.compile(re.escape('&&'))),('%', '=', re.compile(re.escape('%'))),('!', '=', re.compile(re.escape('!'))),('num', ':', re.compile('\d+', re.DOTALL|re.MULTILINE)),('str', ':', re.compile('"(?P<val>[^"]*)"', re.DOTALL|re.MULTILINE)),('id', ':', re.compile('[A-Za-z_][A-Za-z0-9_]*', re.DOTALL|re.MULTILINE)),('ws', '<', re.compile('(\s|\n)+', re.DOTALL|re.MULTILINE)),('com', '<', re.compile('#[^\n]*', re.DOTALL|re.MULTILINE))]

# Return the number of newlines in the match object.
def countlines(match):
    return match.group(0).count('\n')

# Generate tokens given a program string.
def lex(prog):
    linecount = 1

    while len(prog):

        match = None

        # Match token patterns
        for (label, typ, pattern) in triples:

            match = re.match(pattern, prog)

            if match:

                if typ == '<': pass
                elif match.groups('val'):
                    val = match.group('val')
                    end = val.count('\n')
                    yield tok.token(label, val, linecount, linecount+end)
                else:
                    val = prog[:match.end(0)]
                    end = val.count('\n')
                    yield tok.token(label, val, linecount, linecount+end)
                linecount += countlines(match)
                prog = prog[match.end(0):]
                # print('linecount: %s' % linecount)
                break

        # No token patterns matched
        if not match:
            linefrag = ''
            try:
                linefrag = prog[:prog.index('\n')]
            except ValueError:
                linefrag = prog
            raise SyntaxError('line %d: unexpected sequence "%s".'
                              % (linecount, linefrag))